"""
Pulse - Stalled Conversation Resurrection Engine

Streamlit app with three main features:
1. Classify & Nudge: Paste a transcript, get classification and nudge
2. Friction Heatmap: Analyze batch transcripts for drop-off points
3. Review Queue: Approve/Edit/Reject generated nudges
"""

import json
import os
import time
from datetime import datetime
from pathlib import Path

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

from src.models import (
    TranscriptInput,
    Message,
    MessageRole,
    BrandPersona,
    BRAND_PERSONAS,
    StallCategory,
    StallStatus,
    ReviewDecision,
    ClassificationResult,
    NudgeResult,
)
from src.classifier import classify_transcript as _real_classify_transcript
from src.nudge_generator import generate_nudge as _real_generate_nudge
from src.nudge_generator import compare_brand_voices as _real_compare_brand_voices
from src.backend_status import check_backend_status, BackendStatusChecker
from src.friction_report import (
    generate_friction_report,
    generate_friction_report_by_type,
    print_friction_report,
)
from src.database import get_database, PulseDatabase


# =============================================================================
# DEMO MODE CONFIGURATION
# =============================================================================
# Toggle this to False ONLY when recording the final specific-example demo
# When True: Uses cached/mock data, no API calls, instant responses
# When False: Uses real Gemini API (watch for rate limits!)
USE_MOCK_DATA = True

# Path to pre-computed friction data (generated by scripts/batch_precompute.py)
FRICTION_DATA_PATH = Path("data/friction_data.json")


# =============================================================================
# MOCK DATA DEFINITIONS (for demo stability)
# =============================================================================
MOCK_CLASSIFICATIONS = {
    "HIGH_FRICTION:VIN_REQUEST": {
        "status": StallStatus.STALLED_HIGH_RISK,
        "category": StallCategory.HIGH_FRICTION,
        "reason": "HIGH_FRICTION:VIN_REQUEST",
        "confidence": 0.95,
        "evidence": "User stated: 'I'm at work right now. I don't have that on me.'",
    },
    "HIGH_FRICTION:LICENSE_REQUEST": {
        "status": StallStatus.STALLED_HIGH_RISK,
        "category": StallCategory.HIGH_FRICTION,
        "reason": "HIGH_FRICTION:LICENSE_REQUEST",
        "confidence": 0.92,
        "evidence": "User stated inability to provide license information",
    },
    "HIGH_FRICTION:DOCUMENT_UPLOAD": {
        "status": StallStatus.STALLED_HIGH_RISK,
        "category": StallCategory.HIGH_FRICTION,
        "reason": "HIGH_FRICTION:DOCUMENT_UPLOAD",
        "confidence": 0.89,
        "evidence": "User expressed difficulty with document upload request",
    },
    "HIGH_FRICTION:GENERAL": {
        "status": StallStatus.STALLED_LOW_RISK,
        "category": StallCategory.HIGH_FRICTION,
        "reason": "HIGH_FRICTION:GENERAL",
        "confidence": 0.78,
        "evidence": "User expressed difficulty with request",
    },
    "CONFUSION": {
        "status": StallStatus.STALLED_LOW_RISK,
        "category": StallCategory.CONFUSION,
        "reason": "CONFUSION",
        "confidence": 0.85,
        "evidence": "User expressed confusion about terminology",
    },
    "BENIGN": {
        "status": StallStatus.BENIGN,
        "category": StallCategory.BENIGN,
        "reason": "BENIGN",
        "confidence": 0.70,
        "evidence": "User appears to be naturally pausing",
    },
}

MOCK_NUDGES = {
    BrandPersona.HELPFUL_NEIGHBOR: {
        "HIGH_FRICTION:VIN_REQUEST": "No stress on the VIN! A photo of your registration works too, or I can look it up by plate number if you prefer.",
        "HIGH_FRICTION:LICENSE_REQUEST": "No rush! Your license number is also on your insurance card if that's easier to grab.",
        "HIGH_FRICTION:DOCUMENT_UPLOAD": "Totally understand! You can snap a pic whenever's convenient, or I can pull your info another way.",
        "HIGH_FRICTION:GENERAL": "Hey, no worries! Let me know if there's an easier way I can help you out.",
        "CONFUSION": "Oops, my bad for the jargon! In plain English: this just means your coverage if someone without insurance hits you.",
    },
    BrandPersona.PROFESSIONAL_ADVISOR: {
        "HIGH_FRICTION:VIN_REQUEST": "I understand the VIN can be difficult to locate. A photo of your registration document would work equally well.",
        "HIGH_FRICTION:LICENSE_REQUEST": "No rush on the license number. Your current insurance card would contain the same information.",
        "HIGH_FRICTION:DOCUMENT_UPLOAD": "I understand that's inconvenient. Please feel free to send the document when it's convenient for you.",
        "HIGH_FRICTION:GENERAL": "I understand. Please let me know if there's an alternative way I can assist you.",
        "CONFUSION": "I apologize for the unclear terminology. Simply put, this coverage protects you if you're in an accident with an uninsured driver.",
    },
}


# =============================================================================
# CACHED/MOCK WRAPPERS
# =============================================================================
def _detect_friction_type(transcript: TranscriptInput) -> str:
    """Detect friction type from transcript content for mock responses."""
    text = " ".join(m.text.lower() for m in transcript.history)
    
    if "vin" in text:
        return "HIGH_FRICTION:VIN_REQUEST"
    elif "license" in text or "dl" in text:
        return "HIGH_FRICTION:LICENSE_REQUEST"
    elif "photo" in text or "upload" in text or "picture" in text:
        return "HIGH_FRICTION:DOCUMENT_UPLOAD"
    elif any(word in text for word in ["what", "huh", "don't understand", "confused", "mean"]):
        return "CONFUSION"
    elif any(word in text for word in ["busy", "meeting", "later", "think about", "compare"]):
        return "BENIGN"
    else:
        return "HIGH_FRICTION:GENERAL"


@st.cache_data(show_spinner=False)
def classify_transcript_cached(chat_id: str, history_json: str) -> dict:
    """
    Cached wrapper for classify_transcript.
    Uses mock data when USE_MOCK_DATA is True.
    """
    # Reconstruct transcript from cached-friendly inputs
    history = json.loads(history_json)
    transcript = TranscriptInput(
        chat_id=chat_id,
        history=[Message(role=MessageRole(m["role"]), text=m["text"]) for m in history]
    )
    
    if USE_MOCK_DATA:
        # Fake latency to make it feel real
        time.sleep(0.8)
        
        # Detect friction type
        friction_type = _detect_friction_type(transcript)
        mock_data = MOCK_CLASSIFICATIONS.get(friction_type, MOCK_CLASSIFICATIONS["BENIGN"])
        
        return {
            "chat_id": chat_id,
            "status": mock_data["status"].value,
            "category": mock_data["category"].value,
            "reason": mock_data["reason"],
            "confidence": mock_data["confidence"],
            "evidence": mock_data["evidence"],
            "raw_llm_response": "[MOCK MODE]",
            "latency_ms": 800,
        }
    
    # Real API call
    result = _real_classify_transcript(transcript)
    return {
        "chat_id": result.chat_id,
        "status": result.status.value,
        "category": result.category.value,
        "reason": result.reason,
        "confidence": result.confidence,
        "evidence": result.evidence,
        "raw_llm_response": result.raw_llm_response,
        "latency_ms": result.latency_ms,
    }


def classify_transcript(transcript: TranscriptInput) -> ClassificationResult:
    """Wrapper that uses caching and returns proper ClassificationResult."""
    history_json = json.dumps([{"role": m.role.value, "text": m.text} for m in transcript.history])
    cached = classify_transcript_cached(transcript.chat_id, history_json)
    
    return ClassificationResult(
        chat_id=cached["chat_id"],
        status=StallStatus(cached["status"]),
        category=StallCategory(cached["category"]),
        reason=cached["reason"],
        confidence=cached["confidence"],
        evidence=cached["evidence"],
        raw_llm_response=cached["raw_llm_response"],
        latency_ms=cached["latency_ms"],
    )


@st.cache_data(show_spinner=False)
def generate_nudge_cached(chat_id: str, history_json: str, reason: str, brand_persona: str) -> dict:
    """
    Cached wrapper for generate_nudge.
    Uses mock data when USE_MOCK_DATA is True.
    """
    if USE_MOCK_DATA:
        time.sleep(0.5)
        
        persona = BrandPersona(brand_persona)
        nudge_text = MOCK_NUDGES.get(persona, MOCK_NUDGES[BrandPersona.HELPFUL_NEIGHBOR]).get(
            reason, "Thanks for your patience! Let me know when you're ready to continue."
        )
        
        return {
            "nudge_text": nudge_text,
            "raw_llm_response": "[MOCK MODE]",
            "latency_ms": 500,
        }
    
    # Reconstruct for real API call
    history = json.loads(history_json)
    transcript = TranscriptInput(
        chat_id=chat_id,
        history=[Message(role=MessageRole(m["role"]), text=m["text"]) for m in history]
    )
    
    # Need classification for real call
    classification = classify_transcript(transcript)
    result = _real_generate_nudge(transcript, classification, BrandPersona(brand_persona))
    
    return {
        "nudge_text": result.nudge_text,
        "raw_llm_response": result.raw_llm_response,
        "latency_ms": result.latency_ms,
    }


def generate_nudge(
    transcript: TranscriptInput,
    classification: ClassificationResult,
    brand_persona: BrandPersona = BrandPersona.HELPFUL_NEIGHBOR,
) -> NudgeResult:
    """Wrapper that uses caching and returns proper NudgeResult."""
    history_json = json.dumps([{"role": m.role.value, "text": m.text} for m in transcript.history])
    cached = generate_nudge_cached(
        transcript.chat_id, history_json, classification.reason, brand_persona.value
    )
    
    return NudgeResult(
        chat_id=transcript.chat_id,
        classification=classification,
        brand_persona=brand_persona,
        nudge_text=cached["nudge_text"],
        raw_llm_response=cached["raw_llm_response"],
        latency_ms=cached["latency_ms"],
    )


def compare_brand_voices(
    transcript: TranscriptInput,
    classification: ClassificationResult,
) -> dict[BrandPersona, NudgeResult]:
    """Generate nudges in both brand voices for comparison."""
    if USE_MOCK_DATA:
        results = {}
        for persona in BrandPersona:
            results[persona] = generate_nudge(transcript, classification, persona)
        return results
    
    return _real_compare_brand_voices(transcript, classification)


@st.cache_data(show_spinner=False)
def load_precomputed_friction_data() -> dict:
    """Load pre-computed friction data from JSON file."""
    if FRICTION_DATA_PATH.exists():
        with open(FRICTION_DATA_PATH, "r") as f:
            return json.load(f)
    return None


# =============================================================================
# PAGE CONFIG
# =============================================================================
st.set_page_config(
    page_title="Pulse - Conversation Resurrection",
    page_icon="üíì",
    layout="wide",
)

# Custom CSS
st.markdown("""
<style>
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
    .metric-card {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        margin: 10px 0;
    }
    .nudge-box {
        background-color: #e8f4ea;
        border-left: 4px solid #28a745;
        padding: 15px;
        margin: 10px 0;
        border-radius: 5px;
    }
    .warning-box {
        background-color: #fff3cd;
        border-left: 4px solid #ffc107;
        padding: 15px;
        margin: 10px 0;
        border-radius: 5px;
    }
</style>
""", unsafe_allow_html=True)


def parse_transcript_input(text: str) -> TranscriptInput:
    """Parse user input into TranscriptInput format."""
    lines = text.strip().split("\n")
    history = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # Try to parse "BOT: message" or "USER: message" format
        if line.upper().startswith("BOT:"):
            history.append(Message(
                role=MessageRole.BOT,
                text=line[4:].strip()
            ))
        elif line.upper().startswith("USER:"):
            history.append(Message(
                role=MessageRole.USER,
                text=line[5:].strip()
            ))
        elif line.upper().startswith("AGENT:"):
            history.append(Message(
                role=MessageRole.BOT,
                text=line[6:].strip()
            ))
        elif line.upper().startswith("CUSTOMER:"):
            history.append(Message(
                role=MessageRole.USER,
                text=line[9:].strip()
            ))
        else:
            # Try to append to last message or create new one
            if history:
                history[-1].text += " " + line
            else:
                history.append(Message(role=MessageRole.USER, text=line))
    
    return TranscriptInput(
        chat_id=f"manual-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}",
        history=history,
    )


def render_classify_nudge_tab():
    """Render the Classify & Nudge tab."""
    st.header("Classify & Nudge")
    st.markdown("""
    Paste a conversation transcript to classify why the user stalled and generate a contextual nudge.
    """)
    
    # Demo mode indicator
    if USE_MOCK_DATA:
        st.info("üé¨ **Demo Mode Active** ‚Äî Using cached responses for stability. Set `USE_MOCK_DATA = False` for live API.")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("Input Transcript")
        
        # Sample transcript for demo - optimized for perfect result
        sample = """BOT: Hi! I can help you get an auto insurance quote. To get started, I'll need your VIN number.
USER: Ugh, I'm at work right now. I don't have that on me.
BOT: No problem! You can usually find your VIN on your insurance card or registration document."""
        
        transcript_text = st.text_area(
            "Paste transcript (format: BOT: message / USER: message)",
            value=sample,
            height=200,
        )
        
        # Brand voice selection - THE KEY DEMO FEATURE
        st.markdown("---")
        st.markdown("### üé® Brand Voice Control")
        st.markdown("_Since you sell to different carriers, the tone is configurable._")
        
        brand_persona = st.selectbox(
            "Brand Persona",
            options=[bp.value for bp in BrandPersona],
            format_func=lambda x: x.replace("_", " ").title(),
            help="Toggle between personas to see how nudge tone changes instantly"
        )
        
        # Show persona preview
        persona_details = BRAND_PERSONAS[BrandPersona(brand_persona)]
        with st.expander("Persona Details", expanded=False):
            st.markdown(f"**Description:** {persona_details['description']}")
            st.markdown(f"**Example:** _{persona_details['example']}_")
        
        # Multi-channel awareness - THE "DO NO HARM" FEATURE
        st.markdown("---")
        st.markdown("### üõ°Ô∏è Backend Safety Gate")
        st.markdown("_Pulse checks other channels before nudging._")
        
        mock_active_elsewhere = st.checkbox(
            "‚ö†Ô∏è Simulate user active on portal/email",
            value=False,
            help="When checked, simulates a user who is currently active elsewhere ‚Äî nudge will be suppressed"
        )
        
        classify_btn = st.button("Classify & Generate Nudge", type="primary", use_container_width=True)
    
    with col2:
        st.subheader("Results")
        
        if classify_btn and transcript_text:
            try:
                # Parse transcript
                transcript = parse_transcript_input(transcript_text)
                
                if len(transcript.history) < 2:
                    st.error("Please provide at least 2 messages (bot and user)")
                    return
                
                # Check backend status first - THE "DO NO HARM" MOMENT
                with st.spinner("Checking multi-channel status..."):
                    backend_status = check_backend_status(
                        transcript.chat_id,
                        mock_mode=True,
                        mock_active_elsewhere_rate=1.0 if mock_active_elsewhere else 0.0
                    )
                
                # Always show the backend check result
                if backend_status.safe_to_nudge:
                    st.markdown("""
                    <div style="background-color: #d4edda; border-left: 4px solid #28a745; padding: 10px; margin: 10px 0; border-radius: 5px;">
                        <strong>‚úÖ SAFE_TO_NUDGE: TRUE</strong><br>
                        <small>No recent portal/email activity detected. User appears idle.</small>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    # THE KEY "ENTERPRISE SAFETY" MOMENT
                    st.markdown("""
                    <div style="background-color: #f8d7da; border-left: 4px solid #dc3545; padding: 15px; margin: 10px 0; border-radius: 5px;">
                        <strong>üö´ SAFE_TO_NUDGE: FALSE</strong><br>
                        <span style="font-size: 0.95em;">User is currently active in the Portal.</span><br>
                        <strong style="color: #dc3545;">Nudge suppressed ‚Äî sending now would be annoying.</strong>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    with st.expander("Backend Status Details"):
                        st.json({
                            "user_active_elsewhere": backend_status.user_active_elsewhere,
                            "last_portal_activity": str(backend_status.last_portal_activity) if backend_status.last_portal_activity else None,
                            "safe_to_nudge": backend_status.safe_to_nudge,
                            "reason": "User has recent activity on portal within the last 30 minutes"
                        })
                    
                    st.caption("üí° _This proves Pulse understands 'Do No Harm' ‚Äî we don't blindly spam users._")
                    return
                
                # Classify
                with st.spinner("Classifying transcript..."):
                    start = time.time()
                    classification = classify_transcript(transcript)
                    classify_time = time.time() - start
                
                # Display classification
                st.markdown("### Classification")
                
                status_colors = {
                    "STALLED_HIGH_RISK": "üî¥",
                    "STALLED_LOW_RISK": "üü°",
                    "BENIGN": "üü¢",
                }
                
                col_a, col_b, col_c = st.columns(3)
                with col_a:
                    st.metric("Status", f"{status_colors.get(classification.status.value, '')} {classification.status.value}")
                with col_b:
                    st.metric("Category", classification.category.value)
                with col_c:
                    st.metric("Confidence", f"{classification.confidence:.0%}")
                
                st.markdown(f"**Reason:** `{classification.reason}`")
                st.markdown(f"**Evidence:** _{classification.evidence}_")
                st.caption(f"Classification took {classify_time*1000:.0f}ms")
                
                # Generate nudge if not benign
                if classification.category != StallCategory.BENIGN:
                    st.markdown("---")
                    st.markdown("### Suggested Nudge")
                    
                    with st.spinner("Generating nudge..."):
                        start = time.time()
                        nudge = generate_nudge(
                            transcript,
                            classification,
                            BrandPersona(brand_persona),
                        )
                        nudge_time = time.time() - start
                    
                    st.markdown(f"""
                    <div class="nudge-box">
                        <strong>üí¨ {brand_persona.replace('_', ' ').title()}</strong><br>
                        "{nudge.nudge_text}"
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.caption(f"Length: {len(nudge.nudge_text)} chars | Generation took {nudge_time*1000:.0f}ms")
                    
                    # THE KEY "BRAND CONTROL" MOMENT - Show both voices side by side
                    st.markdown("---")
                    st.markdown("### üé® Brand Voice Comparison")
                    st.markdown("_Toggle the Brand Persona dropdown to see how nudges adapt to different carriers._")
                    
                    all_nudges = compare_brand_voices(transcript, classification)
                    
                    voice_col1, voice_col2 = st.columns(2)
                    
                    with voice_col1:
                        neighbor_nudge = all_nudges[BrandPersona.HELPFUL_NEIGHBOR]
                        selected = "**[SELECTED]** " if brand_persona == BrandPersona.HELPFUL_NEIGHBOR.value else ""
                        st.markdown(f"""
                        <div style="background-color: #e3f2fd; border-left: 4px solid #2196f3; padding: 15px; border-radius: 5px;">
                            <strong>{selected}üè† Helpful Neighbor</strong><br>
                            <em style="font-size: 0.9em;">Warm, casual, approachable</em><br><br>
                            "{neighbor_nudge.nudge_text}"
                        </div>
                        """, unsafe_allow_html=True)
                        st.caption(f"{len(neighbor_nudge.nudge_text)} chars")
                    
                    with voice_col2:
                        pro_nudge = all_nudges[BrandPersona.PROFESSIONAL_ADVISOR]
                        selected = "**[SELECTED]** " if brand_persona == BrandPersona.PROFESSIONAL_ADVISOR.value else ""
                        st.markdown(f"""
                        <div style="background-color: #f3e5f5; border-left: 4px solid #9c27b0; padding: 15px; border-radius: 5px;">
                            <strong>{selected}üëî Professional Advisor</strong><br>
                            <em style="font-size: 0.9em;">Formal, polished, reassuring</em><br><br>
                            "{pro_nudge.nudge_text}"
                        </div>
                        """, unsafe_allow_html=True)
                        st.caption(f"{len(pro_nudge.nudge_text)} chars")
                else:
                    st.info("Classification is BENIGN ‚Äî no nudge recommended. User is likely just busy.")
                    
            except Exception as e:
                st.error(f"Error: {str(e)}")
                st.exception(e)


def render_friction_heatmap_tab():
    """Render the Friction Heatmap tab."""
    st.header("Friction Heatmap")
    st.markdown("""
    Analyze batch transcripts to identify which bot questions cause the most user drop-off.
    This acts as a **debugger for your conversation flow**.
    """)
    
    # Check for pre-computed data first
    precomputed = load_precomputed_friction_data()
    
    if precomputed and USE_MOCK_DATA:
        # Use pre-computed data for instant demo
        st.success(f"üìä Loaded pre-computed analysis of **{precomputed['summary']['total_analyzed']}** transcripts")
        
        # Summary metrics - THE KEY DEMO MOMENT
        st.markdown("---")
        st.subheader("Summary")
        
        summary = precomputed["summary"]
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Analyzed", summary["total_analyzed"])
        with col2:
            st.metric("Friction Rate", f"{summary['friction_rate']:.0%}")
        with col3:
            st.metric("High Friction", summary["by_category"].get("HIGH_FRICTION", 0))
        with col4:
            st.metric("Confusion", summary["by_category"].get("CONFUSION", 0))
        
        # THE KILLER CHART - "40% of drop-offs happen at VIN request"
        st.markdown("---")
        st.subheader("üî• Where Users Drop Off")
        
        by_type = precomputed["by_friction_type"]
        if by_type:
            df = pd.DataFrame([
                {
                    "Question Type": k.replace("_", " ").title(),
                    "Friction Rate": v["friction_rate"],
                    "Count": v["total"],
                    "Friction Count": v["friction_count"],
                }
                for k, v in by_type.items()
            ]).sort_values("Friction Rate", ascending=True)  # Ascending for horizontal bar
            
            # Horizontal bar chart - more impactful
            fig = go.Figure(go.Bar(
                x=df["Friction Rate"],
                y=df["Question Type"],
                orientation='h',
                marker=dict(
                    color=df["Friction Rate"],
                    colorscale='RdYlGn_r',
                    showscale=True,
                    colorbar=dict(title="Rate")
                ),
                text=[f"{r:.0%} ({fc} of {c})" for r, fc, c in 
                      zip(df["Friction Rate"], df["Friction Count"], df["Count"])],
                textposition='auto',
            ))
            fig.update_layout(
                height=400,
                xaxis_title="Friction Rate",
                yaxis_title="",
                xaxis=dict(tickformat=".0%"),
                margin=dict(l=20, r=20, t=20, b=20),
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Highlight the top friction point
            if by_type:
                top_type = max(by_type.items(), key=lambda x: x[1]["friction_rate"])
                top_name = top_type[0].replace("_", " ").title()
                top_rate = top_type[1]["friction_rate"]
                
                st.markdown(f"""
                <div style="background-color: #ffe6e6; border-left: 4px solid #dc3545; padding: 15px; margin: 10px 0; border-radius: 5px;">
                    <strong>üö® Key Finding:</strong> <code>{top_name}</code> questions cause <strong>{top_rate:.0%}</strong> of users to stall.
                    <br><em>This isn't just a chatbot feature ‚Äî it's a debugger for your user flow.</em>
                </div>
                """, unsafe_allow_html=True)
        
        # Category breakdown
        st.markdown("---")
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("By Category")
            if summary["by_category"]:
                fig = px.pie(
                    values=list(summary["by_category"].values()),
                    names=list(summary["by_category"].keys()),
                    color_discrete_sequence=px.colors.qualitative.Set2,
                )
                fig.update_layout(height=300)
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("Top Friction Points")
            top_points = precomputed.get("top_friction_points", [])
            if top_points:
                table_data = []
                for fp in top_points[:5]:
                    table_data.append({
                        "Bot Question": fp["question"][:50] + "..." if len(fp["question"]) > 50 else fp["question"],
                        "Rate": f"{fp['friction_rate']:.0%}",
                        "Count": fp["friction_count"],
                    })
                st.dataframe(pd.DataFrame(table_data), use_container_width=True, hide_index=True)
        
        # Actionable insights
        st.markdown("---")
        st.subheader("üí° Actionable Insights")
        
        top_points = precomputed.get("top_friction_points", [])
        if top_points:
            top = top_points[0]
            st.markdown(f"""
            **Top Issue:** "{top['question'][:80]}..."
            
            This question causes **{top['friction_rate']:.0%}** of users to stall ({top['friction_count']} of {top['total']}).
            
            **Recommendations:**
            - Consider offering alternative ways to provide this information
            - Make the request optional if possible (e.g., lookup by license plate)
            - Provide clearer instructions on where to find this data
            - Consider asking for this information later in the flow
            """)
        
        return  # Exit early for pre-computed path
    
    # Fallback: Live analysis (when USE_MOCK_DATA is False or no pre-computed data)
    st.info("üí° **Tip:** Run `python scripts/batch_precompute.py` to pre-compute this data for instant demo loading.")
    
    data_source = st.radio(
        "Data Source",
        ["Use Sample Data", "Upload JSON File"],
        horizontal=True,
    )
    
    transcripts = []
    classifications = []
    
    if data_source == "Use Sample Data":
        if st.button("Load & Analyze Sample Data", type="primary"):
            with st.spinner("Loading sample transcripts and classifying..."):
                # Load sample transcripts
                try:
                    with open("data/sample_transcripts.json", "r") as f:
                        data = json.load(f)
                    with open("data/sample_transcripts_extended.json", "r") as f:
                        data2 = json.load(f)
                    
                    all_transcripts = data.get("transcripts", []) + data2.get("transcripts", [])
                    
                    progress = st.progress(0)
                    for i, t in enumerate(all_transcripts):
                        transcript = TranscriptInput(
                            chat_id=t["chat_id"],
                            history=[
                                Message(role=MessageRole(m["role"]), text=m["text"])
                                for m in t["history"]
                            ]
                        )
                        transcripts.append(transcript)
                        
                        # Classify
                        result = classify_transcript(transcript)
                        classifications.append(result)
                        
                        progress.progress((i + 1) / len(all_transcripts))
                    
                    st.session_state["friction_transcripts"] = transcripts
                    st.session_state["friction_classifications"] = classifications
                    
                except FileNotFoundError:
                    st.error("Sample data files not found. Please ensure data/sample_transcripts.json exists.")
                    return
    else:
        uploaded_file = st.file_uploader("Upload transcripts JSON", type=["json"])
        if uploaded_file:
            data = json.load(uploaded_file)
            # Process uploaded data...
            st.info("Processing uploaded data...")
    
    # Display results if we have data
    if "friction_classifications" in st.session_state:
        transcripts = st.session_state["friction_transcripts"]
        classifications = st.session_state["friction_classifications"]
        
        st.success(f"Analyzed {len(classifications)} transcripts")
        
        # Generate report
        report = generate_friction_report(transcripts, classifications, min_occurrences=1)
        by_type = generate_friction_report_by_type(transcripts, classifications)
        
        # Summary metrics
        st.markdown("---")
        st.subheader("Summary")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Analyzed", report.total_conversations)
        with col2:
            st.metric("Friction Rate", f"{report.overall_friction_rate:.0%}")
        with col3:
            st.metric("High Friction", report.by_category.get("HIGH_FRICTION", 0))
        with col4:
            st.metric("Confusion", report.by_category.get("CONFUSION", 0))
        
        # Charts
        st.markdown("---")
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("By Category")
            if report.by_category:
                fig = px.pie(
                    values=list(report.by_category.values()),
                    names=list(report.by_category.keys()),
                    color_discrete_sequence=px.colors.qualitative.Set2,
                )
                fig.update_layout(height=300)
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("By Question Type")
            if by_type:
                df = pd.DataFrame([
                    {"Type": k, "Friction Rate": v["friction_rate"], "Count": v["total"]}
                    for k, v in by_type.items()
                ]).sort_values("Friction Rate", ascending=False)
                
                fig = px.bar(
                    df,
                    x="Type",
                    y="Friction Rate",
                    color="Friction Rate",
                    color_continuous_scale="RdYlGn_r",
                )
                fig.update_layout(height=300)
                st.plotly_chart(fig, use_container_width=True)
        
        # Top friction points table
        st.markdown("---")
        st.subheader("Top Friction Points")
        st.markdown("_Bot questions causing the most user drop-off:_")
        
        if report.top_friction_points:
            table_data = []
            for fp in report.top_friction_points[:10]:
                table_data.append({
                    "Bot Question": fp.bot_question[:60] + "..." if len(fp.bot_question) > 60 else fp.bot_question,
                    "Occurrences": fp.total_occurrences,
                    "Friction": fp.friction_count,
                    "Rate": f"{fp.friction_rate:.0%}",
                })
            
            st.dataframe(
                pd.DataFrame(table_data),
                use_container_width=True,
                hide_index=True,
            )
        
        # Actionable insights
        st.markdown("---")
        st.subheader("üí° Actionable Insights")
        
        if report.top_friction_points:
            top = report.top_friction_points[0]
            st.markdown(f"""
            **Top Issue:** "{top.bot_question[:80]}..."
            
            This question causes **{top.friction_rate:.0%}** of users to stall ({top.friction_count} out of {top.total_occurrences}).
            
            **Recommendations:**
            - Consider offering alternative ways to provide this information
            - Make the request optional if possible
            - Provide clearer instructions on where to find this data
            - Consider asking for this information later in the flow
            """)


def render_review_queue_tab():
    """Render the Review Queue tab."""
    st.header("Review Queue")
    st.markdown("""
    Review generated nudges before they're sent. Approve, edit, or reject each one.
    """)
    
    db = get_database()
    
    # Stats
    stats = db.get_review_stats()
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Reviewed", stats["total"])
    with col2:
        st.metric("Approved", stats["approved"])
    with col3:
        st.metric("Approval Rate", f"{stats['approval_rate']:.0%}")
    with col4:
        st.metric("Avg Review Time", f"{stats['avg_review_time']:.1f}s")
    
    st.markdown("---")
    
    # Get nudges pending review
    pending = db.get_nudges_for_review(limit=20)
    
    if not pending:
        st.info("No nudges pending review. Generate some nudges from the Classify & Nudge tab first!")
        
        # Demo mode
        if st.button("Generate Demo Nudges"):
            with st.spinner("Generating demo nudges..."):
                # Load sample and generate
                try:
                    with open("data/sample_transcripts.json", "r") as f:
                        data = json.load(f)
                    
                    for t in data["transcripts"][:5]:
                        transcript = TranscriptInput(
                            chat_id=t["chat_id"],
                            history=[
                                Message(role=MessageRole(m["role"]), text=m["text"])
                                for m in t["history"]
                            ]
                        )
                        
                        db.save_transcript(transcript)
                        classification = classify_transcript(transcript)
                        class_id = db.save_classification(classification)
                        
                        if classification.category != StallCategory.BENIGN:
                            nudge = generate_nudge(transcript, classification)
                            db.save_nudge(nudge, class_id)
                    
                    st.rerun()
                except Exception as e:
                    st.error(f"Error: {e}")
        return
    
    st.subheader(f"Pending Review ({len(pending)})")
    
    for item in pending:
        with st.expander(f"üìù {item['chat_id']} - {item['category']}", expanded=False):
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown("**Conversation:**")
                for msg in item["history"]:
                    role = "ü§ñ" if msg["role"] == "bot" else "üë§"
                    st.markdown(f"{role} {msg['text']}")
                
                st.markdown(f"**Classification:** {item['category']} (confidence: {item['confidence']:.0%})")
            
            with col2:
                st.markdown("**Suggested Nudge:**")
                st.markdown(f"""
                <div class="nudge-box">
                    "{item['nudge_text']}"
                </div>
                """, unsafe_allow_html=True)
                st.caption(f"Brand: {item['brand_persona']} | Length: {len(item['nudge_text'])} chars")
                
                # Edit field
                edited = st.text_area(
                    "Edit nudge (optional)",
                    value=item["nudge_text"],
                    key=f"edit_{item['nudge_id']}",
                    height=80,
                )
                
                # Action buttons
                bcol1, bcol2, bcol3 = st.columns(3)
                
                with bcol1:
                    if st.button("‚úÖ Approve", key=f"approve_{item['nudge_id']}"):
                        decision = ReviewDecision.EDITED if edited != item["nudge_text"] else ReviewDecision.APPROVED
                        db.save_review(
                            item["nudge_id"],
                            decision,
                            edited_text=edited if edited != item["nudge_text"] else None,
                        )
                        st.success("Approved!")
                        st.rerun()
                
                with bcol2:
                    if st.button("‚úèÔ∏è Edit & Approve", key=f"edit_approve_{item['nudge_id']}"):
                        db.save_review(
                            item["nudge_id"],
                            ReviewDecision.EDITED,
                            edited_text=edited,
                        )
                        st.success("Edited and approved!")
                        st.rerun()
                
                with bcol3:
                    if st.button("‚ùå Reject", key=f"reject_{item['nudge_id']}"):
                        db.save_review(
                            item["nudge_id"],
                            ReviewDecision.REJECTED,
                        )
                        st.warning("Rejected")
                        st.rerun()


def main():
    """Main app entry point."""
    st.title("üíì Pulse")
    st.markdown("*Stalled Conversation Resurrection Engine*")
    
    # Tabs
    tab1, tab2, tab3 = st.tabs([
        "üîç Classify & Nudge",
        "üî• Friction Heatmap",
        "üìã Review Queue",
    ])
    
    with tab1:
        render_classify_nudge_tab()
    
    with tab2:
        render_friction_heatmap_tab()
    
    with tab3:
        render_review_queue_tab()
    
    # Footer
    st.markdown("---")
    st.caption("Pulse v0.1.0 | Built for General Magic")


if __name__ == "__main__":
    main()
